{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP: Recommendations and Sentiment Analysis\n",
    "\n",
    "We will perform two common NLP tasks: \n",
    " 1. Generate recommendations for products based on product descriptions using an LDA topic model.\n",
    " 2. Perform sentiment analysis based on product reviews using sklearn Pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Generate Recommendations from LDA Transformation\n",
    "\n",
    "I transform a set of product descriptions using TfIdf and LDA topic modeling to generate product recommendations based on similarity in LDA space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and transform text using TfIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   name_title   5000 non-null   object\n",
      " 1   description  5000 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 78.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df_jcp = pd.read_csv('../data/jcpenney-products_subset.csv.zip')\n",
    "\n",
    "print(df_jcp.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invicta® Sl Rally Mens Black Leather Strap Chronograph Watch 16012\n",
      "--------------------------------------------------\n",
      "A timepiece you can enjoy every day of the week, this sports car-inspired chronograph watch packs plenty of information into an easy-to-read dial.   Brand: Invicta Dial Color: Black Strap: Black leather Clasp: Buckle Movement: Quartz Water Resistance: 100m Case Width: 48mm Case Thickness: 13.5mm Bracelet Dimensions: 210mm long; 22mm wide Model No.: 16012 Special Features: Stopwatch; 3 multifunction sub dials   Jewelry photos are enlarged to show detail.\n"
     ]
    }
   ],
   "source": [
    "print(df_jcp.name_title.iloc[0])\n",
    "\n",
    "print('-'*50) \n",
    "\n",
    "\n",
    "print(df_jcp.description.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 5678)\n"
     ]
    }
   ],
   "source": [
    "# Transform Descriptions using TfIdf\n",
    "\n",
    "# Import TfidfVectorizer from sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    \n",
    "#  Instantiate a TfidfVectorizer that will\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=10, max_df=.10)\n",
    "\n",
    "X_tfidf = tfidf.fit_transform(df_jcp.description)\n",
    "\n",
    "print(X_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['jewelry photos', 'features stopwatch', 'special features',\n",
      "       'model no', 'wide model', '22mm wide', 'long 22mm',\n",
      "       'bracelet dimensions', 'case thickness', 'case width',\n",
      "       'resistance 100m', 'water resistance', 'quartz water',\n",
      "       'movement quartz', 'buckle movement', 'clasp buckle',\n",
      "       'leather clasp', 'black leather', 'strap black', 'black strap',\n",
      "       'color black', 'dial color', 'to read', 'easy to', 'an easy',\n",
      "       'plenty of', 'of the', 'day of', 'every day', 'you can', 'sub',\n",
      "       'stopwatch', 'special', 'no', 'model', 'wide', '22mm',\n",
      "       'dimensions', 'bracelet', '5mm', '13', 'thickness', 'width',\n",
      "       'case', '100m', 'resistance', 'water', 'quartz', 'movement',\n",
      "       'buckle', 'clasp', 'leather', 'strap', 'black', 'color', 'brand',\n",
      "       'dial', 'read', 'into', 'plenty', 'watch', 'chronograph',\n",
      "       'inspired', 'car', 'sports', 'week', 'day', 'every', 'enjoy',\n",
      "       'can'], dtype='<U24')]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf.inverse_transform(X_tfidf[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zipper_pocket', 'zipper_pockets', 'zippered', 'zirconia', 'zone']\n"
     ]
    }
   ],
   "source": [
    "# Format Bigrams\n",
    "vocab = tfidf.get_feature_names_out()\n",
    "\n",
    "vocab = [term.replace(' ', '_') for term in vocab]\n",
    "\n",
    "# Printing the last 5 terms\n",
    "print(vocab[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform product descriptions into topics and print sample terms from topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 20)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform Topic Modeling with LDA\n",
    "\n",
    "# I use Latent Direchlet Allocation to learn \n",
    "#   per-document topic distributions and per-topic term distributions.\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Instantiate a LatentDirichletAllocation model \n",
    "lda = LatentDirichletAllocation(n_components=20, n_jobs=-1, random_state=512)\n",
    "\n",
    "X_lda = lda.fit_transform(X_tfidf)\n",
    "\n",
    "X_lda.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta_0 = [0.01 0.74 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01\n",
      " 0.16 0.01 0.01 0.01 0.01 0.01]\n",
      "\n",
      "n_topics_assigned_0 = [0.74, 0.16]\n",
      "\n",
      "assigned_topics_0 = [ 1 14]\n"
     ]
    }
   ],
   "source": [
    "# Get Assigned Topics for Product at df_jcp row 0\n",
    "\n",
    "theta_0 = X_lda[0].round(2)\n",
    "print(f'{theta_0 = :}\\n')\n",
    "\n",
    "# LDA will assign a small weight (or proability) to each topic for a document\n",
    "n_topics_assigned_0 = [topic for topic in theta_0 if topic > 0.01]\n",
    "print(f'{n_topics_assigned_0 = :}\\n')\n",
    "\n",
    "assigned_topics_0 = np.array(np.argsort(theta_0)[::-1])[:len(n_topics_assigned_0)]\n",
    "print(f'{assigned_topics_0 = :}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic # 0 : great_gift edge_to dimensions_18 sale_of edta\n",
      "Topic # 1 : ci seating_arrangement it_does screwdriver_needed screwdriver\n",
      "Topic # 2 : for_proper recommended_yes shirttail ring_with the_dial\n",
      "Topic # 3 : compression_fit yes_use retardant_yes yes_slip garment_should\n",
      "Topic # 4 : vitamin white_gold shaft_circumference are_extremely comforter_shams\n",
      "Topic # 5 : petite_short twin_comforter sodium_benzoate mesh_panels adjustable_cuffs\n",
      "Topic # 6 : solar sunlight_measures alterations crevice crevice_tool\n",
      "Topic # 7 : purchase_this bamboo redness fresh_food refund\n",
      "Topic # 8 : suit_pants and_plywood fit_snugly gold_over fit_straight\n",
      "Topic # 9 : fit_snugly hoop hoop_earrings i1 i2_setting\n",
      "Topic #10 : sits_below seams_to seam_pockets screen_printed savings\n",
      "Topic #11 : care_some elastane_machine chafing hemmed_cotton chain_jewelry\n",
      "Topic #12 : cycles skintone pillowcases_king pillows_are skin_types\n",
      "Topic #13 : and_thighs cotton_comfort booties easy_gift sleeves_regular\n",
      "Topic #14 : require_special some_diamonds vp lab_created alterations\n",
      "Topic #15 : can_purchase and_require overall_thickness and_resistant and_salon\n",
      "Topic #16 : compartments compression_fit complexion plated_10k plates_four\n",
      "Topic #17 : required_phillips cloth_assembly closure_synthetic closure_straight closure_spring\n",
      "Topic #18 : seat_depth aldo rounded_hem booties or_snug\n",
      "Topic #19 : glazed application purchases_to hemmed_cotton purchases\n"
     ]
    }
   ],
   "source": [
    "# Print Top Topic Terms\n",
    "\n",
    "vocab = np.array(vocab)\n",
    "\n",
    "# assert that vocab is the correct datatype\n",
    "assert type(vocab) is np.ndarray, \"vocab needs to be converted to a numpy array\"\n",
    "\n",
    "topic_dist = np.array(np.argsort(lda.components_)[::-1])[:,:5]\n",
    "topic_vocab = vocab[topic_dist]\n",
    "for topic_idx, topic_words in enumerate(topic_vocab): \n",
    "    words = ' '.join(topic_words)\n",
    "    print(f'Topic #{topic_idx:2d} : {words}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate recommendations using topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 5000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate Similarity Matrix\n",
    "\n",
    "# I use Content-Based Filtering to make recommendations based on a query product.\n",
    "# Each product will be represented by its LDA topic weights learned above (X_lda).\n",
    "# I try to recommend similar products in LDA space using cosine distance as our measure of similarity, \n",
    "# where lower distance means more similar.\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "# Use cosine_distances to generate similarity scores on our X_lda data\n",
    "distances = cosine_distances(X_lda)\n",
    "\n",
    "distances.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Invicta® Sl Rally Mens Black Leather Strap Chronograph Watch 16012'\n",
      " 'Seiko® Mens Two-Tone Brown Dial Chronograph Watch SSC142'\n",
      " 'Despicable Me Minions Kids Flashing and Sound Digital Watch'\n",
      " 'Citizen® Eco-Drive® Womens Crystal-Accent Stainless Steel Watch EX1320-54E'\n",
      " 'Womens Crystal-Accent White Lizard Faux Leather Cuff Bangle Watch'\n",
      " 'Star Wars® Stormtrooper Kids Flashing and Sound Digital Watch'\n",
      " 'Casio® Mens Champagne Dial Black Resin Strap Sport Watch MW600F-9AV'\n",
      " 'TKO ORLOGI Womens Crystal-Accent Chain-Link Blue Silicone Strap Stretch Watch'\n",
      " 'Pulsar® Mens Silver-Tone Black Ion Watch PS9273'\n",
      " 'Armitron® ProSport Womens Digital Sport Chronograph Watch 45/7036PNK']\n"
     ]
    }
   ],
   "source": [
    "# Find Recommended Products\n",
    "\n",
    "print(df_jcp.name_title[np.argsort(distances[0])[:10]].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Using Pipelines\n",
    "\n",
    "Here I train a model to classify positive vs negative sentiment on a set of pet supply product reviews using sklearn Pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   review  10000 non-null  object\n",
      " 1   rating  10000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 156.4+ KB\n",
      "None\n",
      "\n",
      "My cats are considerably more happy with this toy...and I don't have to leave the sofa to use it, given the long wand length. yay laziness!!\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# The dataset I am working with is a set of product reviews of pet supply items on Amazon.\n",
    "# This data is taken from https://nijianmo.github.io/amazon/index.html\n",
    "#   \"Justifying recommendations using distantly-labeled reviews and fined-grained aspects\"\n",
    "#   Jianmo Ni, Jiacheng Li, Julian McAuley\n",
    "#   Empirical Methods in Natural Language Processing (EMNLP), 2019\n",
    "\n",
    "df_amzn = pd.read_csv('../data/amazon-petsupply-reviews_subset.csv.zip')\n",
    "\n",
    "print(df_amzn.info())\n",
    "print() \n",
    "print(df_amzn.review[0])\n",
    "print(df_amzn.rating[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rating\n",
      "5    0.66\n",
      "4    0.14\n",
      "3    0.09\n",
      "1    0.06\n",
      "2    0.05\n",
      "dtype: float64\n",
      "\n",
      "True     0.66\n",
      "False    0.34\n",
      "Name: rating, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Transform Target\n",
    "\n",
    "# I turn rating on 5 point scale into a binary classification task to approximate positive vs negative sentiment\n",
    "\n",
    "print(df_amzn.value_counts(subset='rating', normalize=True).round(2))\n",
    "\n",
    "y = df_amzn.rating.replace(to_replace={5:True, 4:False, 3:False, 2:False, 1:False})\n",
    "\n",
    "print()\n",
    "\n",
    "print(y.value_counts(normalize=True).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True     0.66\n",
      "False    0.34\n",
      "Name: rating, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Train-test split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "reviews_train,reviews_test,y_train,y_test = train_test_split(df_amzn.review, y, stratify=y, test_size=0.2, random_state=512)\n",
    "\n",
    "print(y_train.value_counts(normalize=True).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('tfidf', TfidfVectorizer(max_df=0.5, min_df=5)),\n",
      "                ('gbc', GradientBoostingClassifier(n_estimators=20))])\n"
     ]
    }
   ],
   "source": [
    "# Create a Pipeline of TfIdf transformation and Classification\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "pipe_gbc = Pipeline([('tfidf', TfidfVectorizer(min_df=5, max_df=0.50)),('gbc', GradientBoostingClassifier(n_estimators=20))])\n",
    "\n",
    "print(pipe_gbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gbc__max_depth': 10, 'tfidf__ngram_range': (1, 2)}\n",
      "0.75\n"
     ]
    }
   ],
   "source": [
    "# Perform Grid Search on pipe_gbc\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'tfidf__ngram_range':[(1,1), (1,2)], 'gbc__max_depth':[2,10]}\n",
    "\n",
    "gs_pipe_gbc = GridSearchCV(estimator=pipe_gbc, param_grid=param_grid, cv=2, n_jobs=-1).fit(reviews_train, y_train)\n",
    "\n",
    "print(gs_pipe_gbc.best_params_)\n",
    "\n",
    "print(gs_pipe_gbc.best_score_.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.76\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the test set\n",
    "\n",
    "print(gs_pipe_gbc.score(reviews_test,y_test).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True False]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on example reviews\n",
    "\n",
    "print(gs_pipe_gbc.predict(['This is a great product.', 'This product is not great.']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eods-f23",
   "language": "python",
   "name": "eods-f23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
